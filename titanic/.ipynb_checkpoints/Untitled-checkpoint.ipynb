{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age             86\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             1\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age             86\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             1\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['Ticket']\n",
    "del train['Cabin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Sex = train.Sex.replace(['male', 'female'], [0, 1])\n",
    "test.Sex = test.Sex.replace(['male', 'female'], [0, 1])\n",
    "train.Age = train.Age.fillna(train.Age.median())\n",
    "test.Age = test.Age.fillna(test.Age.median())\n",
    "train.Embarked = train.Embarked.fillna(\"S\")\n",
    "\n",
    "train.Embarked = train.Embarked.replace(['C', 'S', 'Q'], [0, 1, 2])\n",
    "test.Embarked = test.Embarked.replace(['C', 'S', 'Q'], [0, 1, 2])\n",
    "test.Fare = test.Fare.fillna(test.Fare.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[\"Survived\"].values\n",
    "X_train = train[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\",\"Fare\",\"Embarked\"]].values\n",
    "\n",
    "X_test = test[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\",\"Fare\",\"Embarked\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping=EarlyStopping(patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 668 samples, validate on 223 samples\n",
      "Epoch 1/500\n",
      "668/668 [==============================] - 1s 761us/step - loss: 55.4112 - val_loss: 4.5130\n",
      "Epoch 2/500\n",
      "668/668 [==============================] - 0s 76us/step - loss: 23.2180 - val_loss: 1.9384\n",
      "Epoch 3/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 9.8200 - val_loss: 0.5692\n",
      "Epoch 4/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 7.6053 - val_loss: 0.7636\n",
      "Epoch 5/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 5.7649 - val_loss: 0.4566\n",
      "Epoch 6/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 2.7724 - val_loss: 0.5107\n",
      "Epoch 7/500\n",
      "668/668 [==============================] - 0s 77us/step - loss: 3.1542 - val_loss: 0.4140\n",
      "Epoch 8/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 1.2629 - val_loss: 0.3935\n",
      "Epoch 9/500\n",
      "668/668 [==============================] - 0s 74us/step - loss: 1.0200 - val_loss: 0.3131\n",
      "Epoch 10/500\n",
      "668/668 [==============================] - 0s 77us/step - loss: 1.8972 - val_loss: 0.2473\n",
      "Epoch 11/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.5334 - val_loss: 0.2331\n",
      "Epoch 12/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.4936 - val_loss: 0.2280\n",
      "Epoch 13/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.4173 - val_loss: 0.2064\n",
      "Epoch 14/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.3486 - val_loss: 0.2021\n",
      "Epoch 15/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.3793 - val_loss: 0.2083\n",
      "Epoch 16/500\n",
      "668/668 [==============================] - 0s 77us/step - loss: 0.2694 - val_loss: 0.2111\n",
      "Epoch 17/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.2728 - val_loss: 0.2028\n",
      "Epoch 18/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.2698 - val_loss: 0.1981\n",
      "Epoch 19/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.3023 - val_loss: 0.2157\n",
      "Epoch 20/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.2373 - val_loss: 0.1962\n",
      "Epoch 21/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.2386 - val_loss: 0.2023\n",
      "Epoch 22/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.2378 - val_loss: 0.2019\n",
      "Epoch 23/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.2332 - val_loss: 0.1995\n",
      "Epoch 24/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.2274 - val_loss: 0.1917\n",
      "Epoch 25/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.2647 - val_loss: 0.1958\n",
      "Epoch 26/500\n",
      "668/668 [==============================] - 0s 78us/step - loss: 0.2248 - val_loss: 0.1921\n",
      "Epoch 27/500\n",
      "668/668 [==============================] - 0s 78us/step - loss: 0.2237 - val_loss: 0.1923\n",
      "Epoch 28/500\n",
      "668/668 [==============================] - 0s 76us/step - loss: 0.2233 - val_loss: 0.1951\n",
      "Epoch 29/500\n",
      "668/668 [==============================] - 0s 74us/step - loss: 0.2351 - val_loss: 0.1921\n",
      "Epoch 30/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.2223 - val_loss: 0.1976\n",
      "Epoch 31/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.2219 - val_loss: 0.1890\n",
      "Epoch 32/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.2299 - val_loss: 0.1897\n",
      "Epoch 33/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.2196 - val_loss: 0.1856\n",
      "Epoch 34/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.2204 - val_loss: 0.1890\n",
      "Epoch 35/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.2197 - val_loss: 0.1890\n",
      "Epoch 36/500\n",
      "668/668 [==============================] - 0s 88us/step - loss: 0.2247 - val_loss: 0.1896\n",
      "Epoch 37/500\n",
      "668/668 [==============================] - 0s 78us/step - loss: 0.2210 - val_loss: 0.1928\n",
      "Epoch 38/500\n",
      "668/668 [==============================] - 0s 83us/step - loss: 0.2207 - val_loss: 0.1866\n",
      "Epoch 39/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.2165 - val_loss: 0.1870\n",
      "Epoch 40/500\n",
      "668/668 [==============================] - 0s 78us/step - loss: 0.2139 - val_loss: 0.1832\n",
      "Epoch 41/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.2260 - val_loss: 0.1915\n",
      "Epoch 42/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.2140 - val_loss: 0.1826\n",
      "Epoch 43/500\n",
      "668/668 [==============================] - 0s 85us/step - loss: 0.2130 - val_loss: 0.1868\n",
      "Epoch 44/500\n",
      "668/668 [==============================] - 0s 80us/step - loss: 0.2232 - val_loss: 0.1900\n",
      "Epoch 45/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.2141 - val_loss: 0.1886\n",
      "Epoch 46/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.2178 - val_loss: 0.1850\n",
      "Epoch 47/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.2066 - val_loss: 0.1860\n",
      "Epoch 48/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.2071 - val_loss: 0.1901\n",
      "Epoch 49/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.2078 - val_loss: 0.1832\n",
      "Epoch 50/500\n",
      "668/668 [==============================] - 0s 78us/step - loss: 0.2045 - val_loss: 0.1829\n",
      "Epoch 51/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.2190 - val_loss: 0.1835\n",
      "Epoch 52/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.2077 - val_loss: 0.1867\n",
      "Epoch 53/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.2004 - val_loss: 0.1801\n",
      "Epoch 54/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.2063 - val_loss: 0.1813\n",
      "Epoch 55/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.2001 - val_loss: 0.1779\n",
      "Epoch 56/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1996 - val_loss: 0.1700\n",
      "Epoch 57/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.2169 - val_loss: 0.1709\n",
      "Epoch 58/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1979 - val_loss: 0.1713\n",
      "Epoch 59/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1924 - val_loss: 0.1634\n",
      "Epoch 60/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1889 - val_loss: 0.1596\n",
      "Epoch 61/500\n",
      "668/668 [==============================] - 0s 74us/step - loss: 0.1920 - val_loss: 0.1686\n",
      "Epoch 62/500\n",
      "668/668 [==============================] - 0s 74us/step - loss: 0.1981 - val_loss: 0.1674\n",
      "Epoch 63/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1873 - val_loss: 0.1653\n",
      "Epoch 64/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1865 - val_loss: 0.1595\n",
      "Epoch 65/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.2036 - val_loss: 0.1557\n",
      "Epoch 66/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1905 - val_loss: 0.1642\n",
      "Epoch 67/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1949 - val_loss: 0.1577\n",
      "Epoch 68/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1904 - val_loss: 0.1584\n",
      "Epoch 69/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1896 - val_loss: 0.1532\n",
      "Epoch 70/500\n",
      "668/668 [==============================] - 0s 76us/step - loss: 0.1908 - val_loss: 0.1564\n",
      "Epoch 71/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1889 - val_loss: 0.1510\n",
      "Epoch 72/500\n",
      "668/668 [==============================] - 0s 77us/step - loss: 0.1863 - val_loss: 0.1529\n",
      "Epoch 73/500\n",
      "668/668 [==============================] - 0s 74us/step - loss: 0.1869 - val_loss: 0.1537\n",
      "Epoch 74/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1783 - val_loss: 0.1520\n",
      "Epoch 75/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1832 - val_loss: 0.1499\n",
      "Epoch 76/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1825 - val_loss: 0.1478\n",
      "Epoch 77/500\n",
      "668/668 [==============================] - 0s 81us/step - loss: 0.1817 - val_loss: 0.1459\n",
      "Epoch 78/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1845 - val_loss: 0.1444\n",
      "Epoch 79/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1757 - val_loss: 0.1594\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668/668 [==============================] - 0s 74us/step - loss: 0.1812 - val_loss: 0.1532\n",
      "Epoch 81/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1738 - val_loss: 0.1505\n",
      "Epoch 82/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1746 - val_loss: 0.1384\n",
      "Epoch 83/500\n",
      "668/668 [==============================] - 0s 77us/step - loss: 0.1727 - val_loss: 0.1408\n",
      "Epoch 84/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1775 - val_loss: 0.1511\n",
      "Epoch 85/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1820 - val_loss: 0.1444\n",
      "Epoch 86/500\n",
      "668/668 [==============================] - 0s 76us/step - loss: 0.1724 - val_loss: 0.1435\n",
      "Epoch 87/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1743 - val_loss: 0.1451\n",
      "Epoch 88/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1799 - val_loss: 0.1519\n",
      "Epoch 89/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1692 - val_loss: 0.1368\n",
      "Epoch 90/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1747 - val_loss: 0.1478\n",
      "Epoch 91/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1822 - val_loss: 0.1407\n",
      "Epoch 92/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1745 - val_loss: 0.1351\n",
      "Epoch 93/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1888 - val_loss: 0.1388\n",
      "Epoch 94/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1871 - val_loss: 0.1389\n",
      "Epoch 95/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1734 - val_loss: 0.1426\n",
      "Epoch 96/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1765 - val_loss: 0.1435\n",
      "Epoch 97/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1744 - val_loss: 0.1345\n",
      "Epoch 98/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1732 - val_loss: 0.1327\n",
      "Epoch 99/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1748 - val_loss: 0.1332\n",
      "Epoch 100/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1739 - val_loss: 0.1335\n",
      "Epoch 101/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1695 - val_loss: 0.1340\n",
      "Epoch 102/500\n",
      "668/668 [==============================] - 0s 74us/step - loss: 0.1633 - val_loss: 0.1324\n",
      "Epoch 103/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1610 - val_loss: 0.1341\n",
      "Epoch 104/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1653 - val_loss: 0.1325\n",
      "Epoch 105/500\n",
      "668/668 [==============================] - 0s 74us/step - loss: 0.1675 - val_loss: 0.1417\n",
      "Epoch 106/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1661 - val_loss: 0.1404\n",
      "Epoch 107/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1653 - val_loss: 0.1368\n",
      "Epoch 108/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1574 - val_loss: 0.1290\n",
      "Epoch 109/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1638 - val_loss: 0.1321\n",
      "Epoch 110/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1624 - val_loss: 0.1267\n",
      "Epoch 111/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1589 - val_loss: 0.1279\n",
      "Epoch 112/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1659 - val_loss: 0.1263\n",
      "Epoch 113/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1611 - val_loss: 0.1278\n",
      "Epoch 114/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1740 - val_loss: 0.1303\n",
      "Epoch 115/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1640 - val_loss: 0.1318\n",
      "Epoch 116/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1627 - val_loss: 0.1310\n",
      "Epoch 117/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1666 - val_loss: 0.1326\n",
      "Epoch 118/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1554 - val_loss: 0.1262\n",
      "Epoch 119/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1599 - val_loss: 0.1339\n",
      "Epoch 120/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1785 - val_loss: 0.1324\n",
      "Epoch 121/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1603 - val_loss: 0.1293\n",
      "Epoch 122/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1609 - val_loss: 0.1292\n",
      "Epoch 123/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1566 - val_loss: 0.1283\n",
      "Epoch 124/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1625 - val_loss: 0.1297\n",
      "Epoch 125/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1598 - val_loss: 0.1265\n",
      "Epoch 126/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1595 - val_loss: 0.1241\n",
      "Epoch 127/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1627 - val_loss: 0.1253\n",
      "Epoch 128/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1567 - val_loss: 0.1271\n",
      "Epoch 129/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1584 - val_loss: 0.1318\n",
      "Epoch 130/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1599 - val_loss: 0.1285\n",
      "Epoch 131/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1666 - val_loss: 0.1292\n",
      "Epoch 132/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1609 - val_loss: 0.1270\n",
      "Epoch 133/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1544 - val_loss: 0.1242\n",
      "Epoch 134/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1580 - val_loss: 0.1270\n",
      "Epoch 135/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1522 - val_loss: 0.1230\n",
      "Epoch 136/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1539 - val_loss: 0.1240\n",
      "Epoch 137/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1614 - val_loss: 0.1279\n",
      "Epoch 138/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1535 - val_loss: 0.1230\n",
      "Epoch 139/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1571 - val_loss: 0.1190\n",
      "Epoch 140/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1574 - val_loss: 0.1301\n",
      "Epoch 141/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1606 - val_loss: 0.1265\n",
      "Epoch 142/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1643 - val_loss: 0.1299\n",
      "Epoch 143/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1544 - val_loss: 0.1194\n",
      "Epoch 144/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1563 - val_loss: 0.1249\n",
      "Epoch 145/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1551 - val_loss: 0.1244\n",
      "Epoch 146/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1527 - val_loss: 0.1195\n",
      "Epoch 147/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1532 - val_loss: 0.1242\n",
      "Epoch 148/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1560 - val_loss: 0.1285\n",
      "Epoch 149/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1562 - val_loss: 0.1272\n",
      "Epoch 150/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1530 - val_loss: 0.1200\n",
      "Epoch 151/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1556 - val_loss: 0.1201\n",
      "Epoch 152/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1496 - val_loss: 0.1225\n",
      "Epoch 153/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1573 - val_loss: 0.1214\n",
      "Epoch 154/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1568 - val_loss: 0.1172\n",
      "Epoch 155/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1602 - val_loss: 0.1184\n",
      "Epoch 156/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1546 - val_loss: 0.1201\n",
      "Epoch 157/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1521 - val_loss: 0.1257\n",
      "Epoch 158/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1522 - val_loss: 0.1227\n",
      "Epoch 159/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668/668 [==============================] - 0s 69us/step - loss: 0.1500 - val_loss: 0.1179\n",
      "Epoch 160/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1524 - val_loss: 0.1237\n",
      "Epoch 161/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1591 - val_loss: 0.1208\n",
      "Epoch 162/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1584 - val_loss: 0.1194\n",
      "Epoch 163/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1515 - val_loss: 0.1161\n",
      "Epoch 164/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1490 - val_loss: 0.1186\n",
      "Epoch 165/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1533 - val_loss: 0.1215\n",
      "Epoch 166/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1539 - val_loss: 0.1196\n",
      "Epoch 167/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1525 - val_loss: 0.1207\n",
      "Epoch 168/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1609 - val_loss: 0.1182\n",
      "Epoch 169/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1530 - val_loss: 0.1221\n",
      "Epoch 170/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1555 - val_loss: 0.1224\n",
      "Epoch 171/500\n",
      "668/668 [==============================] - 0s 67us/step - loss: 0.1506 - val_loss: 0.1174\n",
      "Epoch 172/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1571 - val_loss: 0.1135\n",
      "Epoch 173/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1504 - val_loss: 0.1170\n",
      "Epoch 174/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1518 - val_loss: 0.1216\n",
      "Epoch 175/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1526 - val_loss: 0.1202\n",
      "Epoch 176/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1549 - val_loss: 0.1291\n",
      "Epoch 177/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1510 - val_loss: 0.1202\n",
      "Epoch 178/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1525 - val_loss: 0.1227\n",
      "Epoch 179/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1472 - val_loss: 0.1114\n",
      "Epoch 180/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1491 - val_loss: 0.1148\n",
      "Epoch 181/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1525 - val_loss: 0.1188\n",
      "Epoch 182/500\n",
      "668/668 [==============================] - 0s 67us/step - loss: 0.1516 - val_loss: 0.1244\n",
      "Epoch 183/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1508 - val_loss: 0.1125\n",
      "Epoch 184/500\n",
      "668/668 [==============================] - 0s 74us/step - loss: 0.1484 - val_loss: 0.1099\n",
      "Epoch 185/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1470 - val_loss: 0.1159\n",
      "Epoch 186/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1517 - val_loss: 0.1145\n",
      "Epoch 187/500\n",
      "668/668 [==============================] - 0s 74us/step - loss: 0.1492 - val_loss: 0.1131\n",
      "Epoch 188/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1477 - val_loss: 0.1159\n",
      "Epoch 189/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1470 - val_loss: 0.1177\n",
      "Epoch 190/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1459 - val_loss: 0.1101\n",
      "Epoch 191/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1451 - val_loss: 0.1147\n",
      "Epoch 192/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1534 - val_loss: 0.1139\n",
      "Epoch 193/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1467 - val_loss: 0.1177\n",
      "Epoch 194/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1592 - val_loss: 0.1124\n",
      "Epoch 195/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1526 - val_loss: 0.1212\n",
      "Epoch 196/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1549 - val_loss: 0.1160\n",
      "Epoch 197/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1533 - val_loss: 0.1194\n",
      "Epoch 198/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1503 - val_loss: 0.1129\n",
      "Epoch 199/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1475 - val_loss: 0.1237\n",
      "Epoch 200/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1485 - val_loss: 0.1199\n",
      "Epoch 201/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1515 - val_loss: 0.1203\n",
      "Epoch 202/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1445 - val_loss: 0.1129\n",
      "Epoch 203/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1464 - val_loss: 0.1089\n",
      "Epoch 204/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1474 - val_loss: 0.1120\n",
      "Epoch 205/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1491 - val_loss: 0.1194\n",
      "Epoch 206/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1411 - val_loss: 0.1156\n",
      "Epoch 207/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1462 - val_loss: 0.1133\n",
      "Epoch 208/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1451 - val_loss: 0.1149\n",
      "Epoch 209/500\n",
      "668/668 [==============================] - 0s 81us/step - loss: 0.1471 - val_loss: 0.1142\n",
      "Epoch 210/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1502 - val_loss: 0.1208\n",
      "Epoch 211/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1460 - val_loss: 0.1163\n",
      "Epoch 212/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1446 - val_loss: 0.1121\n",
      "Epoch 213/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1505 - val_loss: 0.1173\n",
      "Epoch 214/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1491 - val_loss: 0.1160\n",
      "Epoch 215/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1484 - val_loss: 0.1191\n",
      "Epoch 216/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1468 - val_loss: 0.1153\n",
      "Epoch 217/500\n",
      "668/668 [==============================] - ETA: 0s - loss: 0.172 - 0s 71us/step - loss: 0.1431 - val_loss: 0.1163\n",
      "Epoch 218/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1453 - val_loss: 0.1162\n",
      "Epoch 219/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1508 - val_loss: 0.1195\n",
      "Epoch 220/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1461 - val_loss: 0.1166\n",
      "Epoch 221/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1426 - val_loss: 0.1281\n",
      "Epoch 222/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1488 - val_loss: 0.1193\n",
      "Epoch 223/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1446 - val_loss: 0.1187\n",
      "Epoch 224/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1509 - val_loss: 0.1189\n",
      "Epoch 225/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1435 - val_loss: 0.1101\n",
      "Epoch 226/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1443 - val_loss: 0.1143\n",
      "Epoch 227/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1534 - val_loss: 0.1107\n",
      "Epoch 228/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1441 - val_loss: 0.1122\n",
      "Epoch 229/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1457 - val_loss: 0.1124\n",
      "Epoch 230/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1503 - val_loss: 0.1145\n",
      "Epoch 231/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1428 - val_loss: 0.1189\n",
      "Epoch 232/500\n",
      "668/668 [==============================] - 0s 76us/step - loss: 0.1481 - val_loss: 0.1159\n",
      "Epoch 233/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1415 - val_loss: 0.1135\n",
      "Epoch 234/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1439 - val_loss: 0.1149\n",
      "Epoch 235/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1447 - val_loss: 0.1079\n",
      "Epoch 236/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1410 - val_loss: 0.1083\n",
      "Epoch 237/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668/668 [==============================] - 0s 72us/step - loss: 0.1429 - val_loss: 0.1090\n",
      "Epoch 238/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1469 - val_loss: 0.1123\n",
      "Epoch 239/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1486 - val_loss: 0.1154\n",
      "Epoch 240/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1430 - val_loss: 0.1128\n",
      "Epoch 241/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1474 - val_loss: 0.1143\n",
      "Epoch 242/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1427 - val_loss: 0.1079\n",
      "Epoch 243/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1426 - val_loss: 0.1217\n",
      "Epoch 244/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1452 - val_loss: 0.1114\n",
      "Epoch 245/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1443 - val_loss: 0.1164\n",
      "Epoch 246/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1452 - val_loss: 0.1107\n",
      "Epoch 247/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1372 - val_loss: 0.1137\n",
      "Epoch 248/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1486 - val_loss: 0.1191\n",
      "Epoch 249/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1452 - val_loss: 0.1121\n",
      "Epoch 250/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1413 - val_loss: 0.1135\n",
      "Epoch 251/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1478 - val_loss: 0.1108\n",
      "Epoch 252/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1421 - val_loss: 0.1094\n",
      "Epoch 253/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1468 - val_loss: 0.1103\n",
      "Epoch 254/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1436 - val_loss: 0.1094\n",
      "Epoch 255/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1376 - val_loss: 0.1125\n",
      "Epoch 256/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1472 - val_loss: 0.1094\n",
      "Epoch 257/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1488 - val_loss: 0.1088\n",
      "Epoch 258/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1390 - val_loss: 0.1089\n",
      "Epoch 259/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1403 - val_loss: 0.1157\n",
      "Epoch 260/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1438 - val_loss: 0.1149\n",
      "Epoch 261/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1401 - val_loss: 0.1087\n",
      "Epoch 262/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1428 - val_loss: 0.1185\n",
      "Epoch 263/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1434 - val_loss: 0.1117\n",
      "Epoch 264/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1429 - val_loss: 0.1112\n",
      "Epoch 265/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1463 - val_loss: 0.1117\n",
      "Epoch 266/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1412 - val_loss: 0.1102\n",
      "Epoch 267/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1497 - val_loss: 0.1120\n",
      "Epoch 268/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1395 - val_loss: 0.1088\n",
      "Epoch 269/500\n",
      "668/668 [==============================] - 0s 81us/step - loss: 0.1453 - val_loss: 0.1100\n",
      "Epoch 270/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1402 - val_loss: 0.1203\n",
      "Epoch 271/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1407 - val_loss: 0.1095\n",
      "Epoch 272/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1377 - val_loss: 0.1129\n",
      "Epoch 273/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1402 - val_loss: 0.1130\n",
      "Epoch 274/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1426 - val_loss: 0.1123\n",
      "Epoch 275/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1385 - val_loss: 0.1112\n",
      "Epoch 276/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1385 - val_loss: 0.1084\n",
      "Epoch 277/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1402 - val_loss: 0.1137\n",
      "Epoch 278/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1384 - val_loss: 0.1121\n",
      "Epoch 279/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1371 - val_loss: 0.1090\n",
      "Epoch 280/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1404 - val_loss: 0.1115\n",
      "Epoch 281/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1450 - val_loss: 0.1104\n",
      "Epoch 282/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1399 - val_loss: 0.1126\n",
      "Epoch 283/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1396 - val_loss: 0.1119\n",
      "Epoch 284/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1473 - val_loss: 0.1146\n",
      "Epoch 285/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1396 - val_loss: 0.1172\n",
      "Epoch 286/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1438 - val_loss: 0.1103\n",
      "Epoch 287/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1402 - val_loss: 0.1105\n",
      "Epoch 288/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1394 - val_loss: 0.1163\n",
      "Epoch 289/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1416 - val_loss: 0.1151\n",
      "Epoch 290/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1355 - val_loss: 0.1077\n",
      "Epoch 291/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1392 - val_loss: 0.1104\n",
      "Epoch 292/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1388 - val_loss: 0.1198\n",
      "Epoch 293/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1407 - val_loss: 0.1067\n",
      "Epoch 294/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1362 - val_loss: 0.1114\n",
      "Epoch 295/500\n",
      "668/668 [==============================] - 0s 74us/step - loss: 0.1368 - val_loss: 0.1075\n",
      "Epoch 296/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1363 - val_loss: 0.1191\n",
      "Epoch 297/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1414 - val_loss: 0.1105\n",
      "Epoch 298/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1366 - val_loss: 0.1103\n",
      "Epoch 299/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1418 - val_loss: 0.1061\n",
      "Epoch 300/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1406 - val_loss: 0.1081\n",
      "Epoch 301/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1368 - val_loss: 0.1094\n",
      "Epoch 302/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1424 - val_loss: 0.1109\n",
      "Epoch 303/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1385 - val_loss: 0.1144\n",
      "Epoch 304/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1454 - val_loss: 0.1096\n",
      "Epoch 305/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1410 - val_loss: 0.1117\n",
      "Epoch 306/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1377 - val_loss: 0.1157\n",
      "Epoch 307/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1386 - val_loss: 0.1161\n",
      "Epoch 308/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1405 - val_loss: 0.1122\n",
      "Epoch 309/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1366 - val_loss: 0.1112\n",
      "Epoch 310/500\n",
      "668/668 [==============================] - 0s 74us/step - loss: 0.1398 - val_loss: 0.1121\n",
      "Epoch 311/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1412 - val_loss: 0.1081\n",
      "Epoch 312/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1368 - val_loss: 0.1057\n",
      "Epoch 313/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1380 - val_loss: 0.1159\n",
      "Epoch 314/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1427 - val_loss: 0.1112\n",
      "Epoch 315/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1394 - val_loss: 0.1069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 316/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1424 - val_loss: 0.1090\n",
      "Epoch 317/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1341 - val_loss: 0.1113\n",
      "Epoch 318/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1355 - val_loss: 0.1138\n",
      "Epoch 319/500\n",
      "668/668 [==============================] - 0s 74us/step - loss: 0.1426 - val_loss: 0.1129\n",
      "Epoch 320/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1401 - val_loss: 0.1110\n",
      "Epoch 321/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1390 - val_loss: 0.1091\n",
      "Epoch 322/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1365 - val_loss: 0.1110\n",
      "Epoch 323/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1453 - val_loss: 0.1088\n",
      "Epoch 324/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1323 - val_loss: 0.1109\n",
      "Epoch 325/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1395 - val_loss: 0.1175\n",
      "Epoch 326/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1385 - val_loss: 0.1091\n",
      "Epoch 327/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1334 - val_loss: 0.1141\n",
      "Epoch 328/500\n",
      "668/668 [==============================] - 0s 74us/step - loss: 0.1365 - val_loss: 0.1126\n",
      "Epoch 329/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1384 - val_loss: 0.1172\n",
      "Epoch 330/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1451 - val_loss: 0.1155\n",
      "Epoch 331/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1341 - val_loss: 0.1149\n",
      "Epoch 332/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1402 - val_loss: 0.1297\n",
      "Epoch 333/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1315 - val_loss: 0.1188\n",
      "Epoch 334/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1386 - val_loss: 0.1177\n",
      "Epoch 335/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1370 - val_loss: 0.1120\n",
      "Epoch 336/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1400 - val_loss: 0.1121\n",
      "Epoch 337/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1388 - val_loss: 0.1273\n",
      "Epoch 338/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1395 - val_loss: 0.1144\n",
      "Epoch 339/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1385 - val_loss: 0.1160\n",
      "Epoch 340/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1382 - val_loss: 0.1144\n",
      "Epoch 341/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1413 - val_loss: 0.1093\n",
      "Epoch 342/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1335 - val_loss: 0.1054\n",
      "Epoch 343/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1339 - val_loss: 0.1219\n",
      "Epoch 344/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1355 - val_loss: 0.1196\n",
      "Epoch 345/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1393 - val_loss: 0.1143\n",
      "Epoch 346/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1376 - val_loss: 0.1094\n",
      "Epoch 347/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1384 - val_loss: 0.1143\n",
      "Epoch 348/500\n",
      "668/668 [==============================] - 0s 78us/step - loss: 0.1388 - val_loss: 0.1147\n",
      "Epoch 349/500\n",
      "668/668 [==============================] - 0s 79us/step - loss: 0.1385 - val_loss: 0.1159\n",
      "Epoch 350/500\n",
      "668/668 [==============================] - 0s 81us/step - loss: 0.1391 - val_loss: 0.1105\n",
      "Epoch 351/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1379 - val_loss: 0.1097\n",
      "Epoch 352/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1390 - val_loss: 0.1074\n",
      "Epoch 353/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1344 - val_loss: 0.1126\n",
      "Epoch 354/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1261 - val_loss: 0.1070\n",
      "Epoch 355/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1388 - val_loss: 0.1103\n",
      "Epoch 356/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1357 - val_loss: 0.1086\n",
      "Epoch 357/500\n",
      "668/668 [==============================] - 0s 74us/step - loss: 0.1368 - val_loss: 0.1198\n",
      "Epoch 358/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1381 - val_loss: 0.1115\n",
      "Epoch 359/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1380 - val_loss: 0.1175\n",
      "Epoch 360/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1361 - val_loss: 0.1187\n",
      "Epoch 361/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1339 - val_loss: 0.1098\n",
      "Epoch 362/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1340 - val_loss: 0.1097\n",
      "Epoch 363/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1347 - val_loss: 0.1092\n",
      "Epoch 364/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1399 - val_loss: 0.1164\n",
      "Epoch 365/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1350 - val_loss: 0.1155\n",
      "Epoch 366/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1371 - val_loss: 0.1100\n",
      "Epoch 367/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1353 - val_loss: 0.1184\n",
      "Epoch 368/500\n",
      "668/668 [==============================] - 0s 72us/step - loss: 0.1345 - val_loss: 0.1120\n",
      "Epoch 369/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1337 - val_loss: 0.1066\n",
      "Epoch 370/500\n",
      "668/668 [==============================] - 0s 81us/step - loss: 0.1362 - val_loss: 0.1078\n",
      "Epoch 371/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1344 - val_loss: 0.1133\n",
      "Epoch 372/500\n",
      "668/668 [==============================] - 0s 68us/step - loss: 0.1349 - val_loss: 0.1150\n",
      "Epoch 373/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1328 - val_loss: 0.1150\n",
      "Epoch 374/500\n",
      "668/668 [==============================] - 0s 74us/step - loss: 0.1360 - val_loss: 0.1113\n",
      "Epoch 375/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1338 - val_loss: 0.1105\n",
      "Epoch 376/500\n",
      "668/668 [==============================] - 0s 76us/step - loss: 0.1368 - val_loss: 0.1108\n",
      "Epoch 377/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1368 - val_loss: 0.1119\n",
      "Epoch 378/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1320 - val_loss: 0.1188\n",
      "Epoch 379/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1361 - val_loss: 0.1084\n",
      "Epoch 380/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1389 - val_loss: 0.1086\n",
      "Epoch 381/500\n",
      "668/668 [==============================] - 0s 77us/step - loss: 0.1357 - val_loss: 0.1111\n",
      "Epoch 382/500\n",
      "668/668 [==============================] - 0s 85us/step - loss: 0.1374 - val_loss: 0.1083\n",
      "Epoch 383/500\n",
      "668/668 [==============================] - 0s 71us/step - loss: 0.1384 - val_loss: 0.1125\n",
      "Epoch 384/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1340 - val_loss: 0.1131\n",
      "Epoch 385/500\n",
      "668/668 [==============================] - 0s 85us/step - loss: 0.1322 - val_loss: 0.1109\n",
      "Epoch 386/500\n",
      "668/668 [==============================] - 0s 75us/step - loss: 0.1376 - val_loss: 0.1108\n",
      "Epoch 387/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1350 - val_loss: 0.1113\n",
      "Epoch 388/500\n",
      "668/668 [==============================] - 0s 70us/step - loss: 0.1358 - val_loss: 0.1179\n",
      "Epoch 389/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1379 - val_loss: 0.1178\n",
      "Epoch 390/500\n",
      "668/668 [==============================] - 0s 73us/step - loss: 0.1400 - val_loss: 0.1082\n",
      "Epoch 391/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1353 - val_loss: 0.1087\n",
      "Epoch 392/500\n",
      "668/668 [==============================] - 0s 69us/step - loss: 0.1332 - val_loss: 0.1147\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs =500, batch_size =20, verbose=1, validation_split=0.25, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['loss', 'val_loss'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.read_csv(\"test.csv\")\n",
    "df_out[\"Survived\"]=y_predict.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out[[\"PassengerId\",\"Survived\"]].to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
